{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T05:59:56.218719Z",
     "start_time": "2019-06-18T05:59:55.749301Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T05:59:56.224304Z",
     "start_time": "2019-06-18T05:59:56.220168Z"
    }
   },
   "outputs": [],
   "source": [
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, csv_path, max_length, input_length, characters):\n",
    "        super(CaptchaDataset, self).__init__()\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.max_length = max_length\n",
    "        self.characters = characters\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.df.filename[index])\n",
    "        image = to_tensor(image)\n",
    "        target = self.encode(self.df.label[index])\n",
    "        input_length = torch.full(size=(1, ), fill_value=self.input_length, dtype=torch.long)\n",
    "        target_length = torch.full(size=(1, ), fill_value=len(self.df.label[index]), dtype=torch.long)\n",
    "        return image, target, input_length, target_length\n",
    "    \n",
    "    def encode(self, label):\n",
    "        target = torch.zeros(size=(self.max_length, ), dtype=torch.long)\n",
    "        for i,c in enumerate(label):\n",
    "            target[i] = self.characters.find(c)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T05:59:56.966311Z",
     "start_time": "2019-06-18T05:59:56.893711Z"
    }
   },
   "outputs": [],
   "source": [
    "characters = ' (0+)9=*867154-32'\n",
    "n_classes = len(characters)\n",
    "batch_size = 128\n",
    "\n",
    "dataset = CaptchaDataset(csv_path='train.csv', max_length=11, input_length=37, characters=characters)\n",
    "train_set, valid_set = random_split(dataset, [80000, 20000])\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T05:59:57.154608Z",
     "start_time": "2019-06-18T05:59:57.152411Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1507.05717.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T05:59:57.531927Z",
     "start_time": "2019-06-18T05:59:57.504956Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            # inputs = (32, 3, 64, 300)\n",
    "            # in_channels, out_channels(卷积核数量), kernel_size, stride, padding\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #[32, 32, 32, 150]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # [32, 64, 16, 75]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # [32, 128, 16, 75]\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # [32, 128, 8, 37]\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            # [32, 256, 4, 37]\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            # [32, 256, 2, 37]\n",
    "                      \n",
    "            nn.Conv2d(256,256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            # [32, 256, 1, 37]\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=256, num_layers=1, bidirectional=True)\n",
    "        self.fc = nn.Linear(in_features=512, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.squeeze(2)       # [32, 256, 37]\n",
    "        x = x.permute(2, 0, 1) # [37, 32, 256]\n",
    "        x, _ = self.lstm(x)    # [37, 32, 512]\n",
    "        x = self.fc(x)         # [37, 32, 17]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T05:59:58.354828Z",
     "start_time": "2019-06-18T05:59:58.049448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 32, 17])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(n_classes)\n",
    "inputs = torch.zeros((32, 3, 64, 300))\n",
    "outputs = model(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T06:00:00.626985Z",
     "start_time": "2019-06-18T05:59:58.596420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU()\n",
       "    (18): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU()\n",
       "    (26): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(256, 256, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(n_classes)\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T06:00:01.170317Z",
     "start_time": "2019-06-18T06:00:01.162928Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_target(out):\n",
    "    tar = ''.join([characters[x] for x in out])\n",
    "    return tar.replace(' ', '')\n",
    "    \n",
    "def decode(out):\n",
    "    pre = ''.join([characters[x] for x in out])\n",
    "    new_pre = ''\n",
    "    for i, x in enumerate(pre[:-1]):\n",
    "        if(x != pre[i+1]):\n",
    "            new_pre += x\n",
    "    \n",
    "    if len(new_pre) < 1:\n",
    "        return ''\n",
    "    if(new_pre[-1] != pre[-1]):\n",
    "        new_pre += pre[-1]\n",
    "    new_pre = new_pre.replace(' ', '')\n",
    "    return new_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T06:15:31.610259Z",
     "start_time": "2019-06-18T06:00:01.677017Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.2940 Acc: 0.4270 : 100%|██████████| 625/625 [01:21<00:00,  7.74it/s]\n",
      "Valid: 1 Loss: 0.4830 Acc: 0.2047 : 100%|██████████| 157/157 [00:11<00:00, 14.22it/s]\n",
      "Epoch: 2 Loss: 0.0568 Acc: 0.8892 : 100%|██████████| 625/625 [01:21<00:00,  7.75it/s]\n",
      "Valid: 2 Loss: 0.7936 Acc: 0.0969 : 100%|██████████| 157/157 [00:10<00:00, 14.39it/s]\n",
      "Epoch: 3 Loss: 0.0127 Acc: 0.9719 : 100%|██████████| 625/625 [01:21<00:00,  7.71it/s]\n",
      "Valid: 3 Loss: 0.0250 Acc: 0.9448 : 100%|██████████| 157/157 [00:10<00:00, 14.30it/s]\n",
      "Epoch: 4 Loss: 0.0088 Acc: 0.9801 : 100%|██████████| 625/625 [01:21<00:00,  7.54it/s]\n",
      "Valid: 4 Loss: 0.1720 Acc: 0.6711 : 100%|██████████| 157/157 [00:10<00:00, 14.49it/s]\n",
      "Epoch: 5 Loss: 0.0085 Acc: 0.9840 : 100%|██████████| 625/625 [01:22<00:00,  7.74it/s]\n",
      "Valid: 5 Loss: 0.0126 Acc: 0.9734 : 100%|██████████| 157/157 [00:10<00:00, 14.29it/s]\n",
      "Epoch: 6 Loss: 0.0044 Acc: 0.9920 : 100%|██████████| 625/625 [01:22<00:00,  7.81it/s]\n",
      "Valid: 6 Loss: 0.0205 Acc: 0.9579 : 100%|██████████| 157/157 [00:10<00:00, 14.60it/s]\n",
      "Epoch: 7 Loss: 0.0025 Acc: 0.9965 : 100%|██████████| 625/625 [01:22<00:00,  7.59it/s]\n",
      "Valid: 7 Loss: 0.0075 Acc: 0.9832 : 100%|██████████| 157/157 [00:10<00:00, 14.40it/s]\n",
      "Epoch: 8 Loss: 0.0022 Acc: 0.9945 : 100%|██████████| 625/625 [01:22<00:00,  7.71it/s]\n",
      "Valid: 8 Loss: 0.0194 Acc: 0.9607 : 100%|██████████| 157/157 [00:11<00:00, 14.25it/s]\n",
      "Epoch: 9 Loss: 0.0016 Acc: 0.9974 : 100%|██████████| 625/625 [01:22<00:00,  7.44it/s]\n",
      "Valid: 9 Loss: 0.0106 Acc: 0.9793 : 100%|██████████| 157/157 [00:11<00:00, 14.27it/s]\n",
      "Epoch: 10 Loss: 0.0008 Acc: 0.9996 : 100%|██████████| 625/625 [01:22<00:00,  7.57it/s]\n",
      "Valid: 10 Loss: 0.0074 Acc: 0.9850 : 100%|██████████| 157/157 [00:10<00:00, 14.53it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters())\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    with tqdm(train_loader) as pbar:\n",
    "        loss_new = 0\n",
    "        acc_new = 0\n",
    "        for batch_index, (data, target, input_lengths, target_lengths) in enumerate(pbar):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            \n",
    "            output_log_softmax = F.log_softmax(output, dim=-1)\n",
    "            loss = F.ctc_loss(output_log_softmax, target, input_lengths, target_lengths)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss = loss.item()\n",
    "            output_argmax = output.detach().argmax(dim=-1).permute(1, 0).cpu()\n",
    "            acc = sum([decode_target(true) == decode(pred) \n",
    "                       for true, pred in zip(target, output_argmax)]) / len(target)\n",
    "\n",
    "            if(batch_index==0):\n",
    "                loss_new = loss\n",
    "                acc_new = acc\n",
    "                \n",
    "            loss_new = 0.1*loss + 0.9*loss_new\n",
    "            acc_new = 0.1*acc + 0.9*acc_new\n",
    "            pbar.set_description(f'Epoch: {epoch} Loss: {loss_new:.4f} Acc: {acc_new:.4f} ')\n",
    "    \n",
    "    model.eval()\n",
    "    with tqdm(valid_loader) as pbar:\n",
    "        loss_sum = 0\n",
    "        acc_sum = 0\n",
    "        for batch_index, (data, target, input_lengths, target_lengths) in enumerate(pbar):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            output_log_softmax = F.log_softmax(output, dim=-1)\n",
    "            loss = F.ctc_loss(output_log_softmax, target, input_lengths, target_lengths)\n",
    "            loss = loss.item()\n",
    "            output_argmax = output.detach().argmax(dim=-1).permute(1, 0).cpu()\n",
    "            acc = sum([decode_target(true) == decode(pred) \n",
    "                       for true, pred in zip(target, output_argmax)]) / len(target)\n",
    "            loss_sum += loss\n",
    "            acc_sum += acc\n",
    "            pbar.set_description(f'Valid: {epoch} Loss: {loss_sum / (batch_index + 1):.4f} '\n",
    "                                 f'Acc: {acc_sum / (batch_index + 1):.4f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:59:36.163183Z",
     "start_time": "2019-06-17T16:59:36.117105Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
